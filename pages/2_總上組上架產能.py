import io
import re
import uuid
import datetime as dt
from typing import Dict, List, Tuple, Optional

import streamlit as st
import pandas as pd

from common_ui import (
    inject_logistics_theme,
    set_page,
    KPI,
    render_kpis,
    bar_topN,
    card_open,
    card_close,
    download_excel,
)

from audit_store import sha256_bytes, upload_export_bytes, insert_audit_run


# =========================
# ä¾ç…§ä½  v8.9 æª”æ¡ˆçš„è¦å‰‡
# =========================
TO_EXCLUDE_KEYWORDS = ["CGS", "JCPL", "QC99", "GREAT0001X", "GX010", "PD99"]
TO_EXCLUDE_PATTERN = re.compile("|".join(re.escape(k) for k in TO_EXCLUDE_KEYWORDS), flags=re.IGNORECASE)

INPUT_USER_CANDIDATES = ["è¨˜éŒ„è¼¸å…¥äºº", "è¨˜éŒ„è¼¸å…¥è€…", "å»ºç«‹äºº", "è¼¸å…¥äºº"]
REV_DT_CANDIDATES = ["ä¿®è¨‚æ—¥æœŸ", "ä¿®è¨‚æ™‚é–“", "ä¿®è¨‚æ—¥", "ç•°å‹•æ™‚é–“", "ä¿®æ”¹æ™‚é–“"]

TARGET_EFF = 20
IDLE_MIN_THRESHOLD = 10
AM_START, AM_END = dt.time(7, 0, 0), dt.time(12, 30, 0)
PM_START, PM_END = dt.time(13, 30, 0), dt.time(23, 59, 59)

NAME_MAP = {
    "20200924001": "é»ƒé›…å›", "20210805001": "éƒ­ä¸­åˆ", "20220505002": "é˜®æ–‡é’æ˜",
    "20221221001": "é˜®æ–‡å…¨", "20221222005": "è¬å¿ é¾", "20230119001": "é™¶æ˜¥é’",
    "20240926001": "é™³è‰å¨œ", "20241011002": "æ—é›™æ…§", "20250502001": "å³è©©æ•",
    "20250617001": "é˜®æ–‡è­š", "20250617003": "å–¬å®¶å¯¶", "20250901009": "å¼µå¯¶è±",
    "G01": "0", "20201109003": "å³æŒ¯å‡±", "09963": "é»ƒè¬™å‡±",
    "20240313003": "é˜®æ›°å¿ ", "20201109001": "æ¢å† å¦‚", "10003": "æèŒ‚éŠ“",
    "20200922002": "è‘‰æ¬²å¼˜", "20250923019": "é˜®æ°ç´…æ·±", "9963": "é»ƒè¬™å‡±",
    "11399": "é™³å“²æ²…",
}

BREAK_RULES = [
    (dt.time(20,45,0), dt.time(22,30,0),  0, "é¦–â‰¥20:45 ä¸” æœ«â‰¤22:30 â†’ 0 åˆ†é˜"),
    (dt.time(18,30,0), dt.time(20,30,0),  0, "é¦–â‰¥18:30 ä¸” æœ«â‰¤20:30 â†’ 0 åˆ†é˜"),
    (dt.time(15,30,0), dt.time(18, 0,0),  0, "é¦–â‰¥15:30 ä¸” æœ«â‰¤18:00 â†’ 0 åˆ†é˜"),
    (dt.time(13,30,0), dt.time(15,35,0),  0, "é¦–â‰¥13:30 ä¸” æœ«â‰¤15:35 â†’ 0 åˆ†é˜"),
    (dt.time(20,45,0), dt.time(23, 0,0),  0, "é¦–â‰¥20:45 ä¸” æœ«â‰¤23:00 â†’ 0 åˆ†é˜"),
    (dt.time(20, 0,0), dt.time(22, 0,0), 15, "é¦–â‰¥20:00 ä¸” æœ«â‰¤22:00 â†’ 15 åˆ†é˜"),
    (dt.time(18,30,0), dt.time(22, 0,0), 15, "é¦–â‰¥18:30 ä¸” æœ«â‰¤22:00 â†’ 15 åˆ†é˜"),
    (dt.time(19, 0,0), dt.time(22,30,0), 15, "é¦–â‰¥19:00 ä¸” æœ«â‰¤22:30 â†’ 15 åˆ†é˜"),
    (dt.time(13,30,0), dt.time(18, 0,0), 15, "é¦–â‰¥13:30 ä¸” æœ«â‰¤18:00 â†’ 15 åˆ†é˜"),
    (dt.time(16, 0,0), dt.time(20,40,0), 30, "é¦–â‰¥16:00 ä¸” æœ«â‰¤20:40 â†’ 30 åˆ†é˜"),
    (dt.time(15,30,0), dt.time(20,30,0), 30, "é¦–â‰¥15:30 ä¸” æœ«â‰¤20:30 â†’ 30 åˆ†é˜"),
    (dt.time(17, 0,0), dt.time(22,30,0), 45, "é¦–â‰¥17:00 ä¸” æœ«â‰¤22:30 â†’ 45 åˆ†é˜"),
    (dt.time(15,45,0), dt.time(22,30,0), 45, "é¦–â‰¥15:45 ä¸” æœ«â‰¤22:30 â†’ 45 åˆ†é˜"),
    (dt.time(13,30,0), dt.time(20,29,0), 45, "é¦–â‰¥13:30 ä¸” æœ«â‰¤20:29 â†’ 45 åˆ†é˜"),
    (dt.time(13,30,0), dt.time(23, 0,0), 60, "é¦–â‰¥13:30 ä¸” æœ«â‰¤23:00 â†’ 60 åˆ†é˜"),
    (dt.time(11, 0,0), dt.time(17, 0,0), 75, "é¦–â‰¥11:00 ä¸” æœ«â‰¤17:00 â†’ 75 åˆ†é˜"),
    (dt.time( 8, 0,0), dt.time(17, 0,0), 90, "é¦–â‰¥08:00 ä¸” æœ«â‰¤17:00 â†’ 90 åˆ†é˜"),
    (dt.time(10,50,0), dt.time(23, 0,0),120, "é¦–â‰¥10:50 ä¸” æœ«â‰¤23:00 â†’ 120 åˆ†é˜"),
    (dt.time( 8, 0,0), dt.time(23, 0,0),135, "é¦–â‰¥08:00 ä¸” æœ«â‰¤23:00 â†’ 135 åˆ†é˜"),
]

EXCLUDE_IDLE_RANGES = [
    (dt.time(10, 0, 0), dt.time(10, 15, 0)),
    (dt.time(12,30, 0), dt.time(13, 30, 0)),
    (dt.time(15,30, 0), dt.time(15, 45, 0)),
    (dt.time(18, 0, 0), dt.time(18, 30, 0)),
    (dt.time(20,30, 0), dt.time(20, 45, 0)),
]


# =========================
# å·¥å…·
# =========================
def _strip_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    return df

def find_first_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    cols = {str(c).strip(): c for c in df.columns}
    for name in candidates:
        if name in cols:
            return name
    norm = {re.sub(r"[ï¼ˆï¼‰\(\)\s]", "", k): k for k in cols}
    for name in candidates:
        key = re.sub(r"[ï¼ˆï¼‰\(\)\s]", "", name)
        if key in norm:
            return norm[key]
    return None

def normalize_to_qc(series: pd.Series) -> pd.Series:
    s = series.astype(str).str.strip().str.upper()
    return s.eq("QC")

def to_not_excluded_mask(series: pd.Series) -> pd.Series:
    s = series.astype(str).str.strip()
    return ~s.str.contains(TO_EXCLUDE_PATTERN, na=False)

def prepare_filtered_df(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame()
    df = _strip_cols(df)
    if "ç”±" not in df.columns or "åˆ°" not in df.columns:
        return pd.DataFrame()
    return df[normalize_to_qc(df["ç”±"]) & to_not_excluded_mask(df["åˆ°"])].copy()

def break_minutes_for_span(first_dt: pd.Timestamp, last_dt: pd.Timestamp) -> Tuple[int, str]:
    if pd.isna(first_dt) or pd.isna(last_dt):
        return 0, "ç„¡æ™‚é–“è³‡æ–™"
    stt, edt = first_dt.time(), last_dt.time()
    for st_ge, ed_le, mins, tag in BREAK_RULES:
        if (stt >= st_ge) and (edt <= ed_le):
            return int(mins), str(tag)
    return 0, "æœªå‘½ä¸­è¦å‰‡"

def _subtract_exclusions(s_dt: pd.Timestamp, e_dt: pd.Timestamp, exclude_ranges):
    # v8.9 ä¿®æ­£ç‰ˆæœ¬ï¼šå®Œæ•´åˆ‡æ‰é‡ç–Šéƒ¨åˆ†
    if s_dt >= e_dt or not exclude_ranges:
        return [(s_dt, e_dt)]
    segments = [(s_dt, e_dt)]
    for ex_s_t, ex_e_t in exclude_ranges:
        ex_s = pd.Timestamp.combine(s_dt.date(), ex_s_t)
        ex_e = pd.Timestamp.combine(s_dt.date(), ex_e_t)
        new_segments = []
        for a, b in segments:
            if b <= ex_s or a >= ex_e:
                new_segments.append((a, b))
            else:
                if a < ex_s:
                    new_segments.append((a, ex_s))
                if b > ex_e:
                    new_segments.append((ex_e, b))
        segments = [(x, y) for (x, y) in new_segments if x < y]
    return segments

def _compute_idle(series_dt: pd.Series, min_minutes=IDLE_MIN_THRESHOLD, exclude_ranges=EXCLUDE_IDLE_RANGES) -> Tuple[int, str]:
    if series_dt.size < 2:
        return 0, ""
    s = series_dt.sort_values()
    total_min, ranges_txt = 0, []
    prev = s.iloc[0]
    for cur in s.iloc[1:]:
        if cur <= prev:
            prev = cur
            continue
        for a, b in _subtract_exclusions(prev, cur, exclude_ranges or []):
            gap_min = int(round((b - a).total_seconds() / 60.0))
            if gap_min >= min_minutes:
                total_min += gap_min
                ranges_txt.append(f"{a.time()} ~ {b.time()}")
        prev = cur
    return int(total_min), "ï¼›".join(ranges_txt)

def _span_metrics(series_dt: pd.Series):
    if series_dt.empty:
        return pd.NaT, pd.NaT, 0
    return series_dt.min(), series_dt.max(), int(series_dt.size)

def compute_am_pm_for_group(g: pd.DataFrame) -> pd.Series:
    times = g["__dt__"]

    # ä¸Šåˆï¼š07:00â€“12:30ï¼ˆä¸æ‰£ä¼‘ï¼‰
    t_am = times[times.dt.time.between(AM_START, AM_END)]
    am_first, am_last, am_cnt = _span_metrics(t_am)
    am_mins = int(round(((am_last - am_first).total_seconds() / 60.0))) if am_cnt > 0 else 0
    am_eff = round((am_cnt / am_mins * 60.0), 2) if am_mins > 0 else 0.0
    am_idle_min, am_idle_ranges = _compute_idle(t_am)

    # ä¸‹åˆï¼š13:30â€“23:59:59ï¼ˆä¾è¦å‰‡æ‰£ä¼‘è¨ˆç®—å·¥æ™‚ï¼‰
    t_pm = times[times.dt.time.between(PM_START, PM_END)]
    pm_first, pm_last, pm_cnt = _span_metrics(t_pm)
    if pm_cnt > 0:
        pm_break, pm_rule = break_minutes_for_span(pm_first, pm_last)
        raw_pm_mins = (pm_last - pm_first).total_seconds() / 60.0
        pm_mins = max(int(round(raw_pm_mins - pm_break)), 0)
    else:
        pm_break, pm_rule, pm_mins = 0, "ç„¡æ™‚é–“è³‡æ–™", 0
    pm_eff = round((pm_cnt / pm_mins * 60.0), 2) if pm_mins > 0 else 0.0
    pm_idle_min, pm_idle_ranges = _compute_idle(t_pm)

    # æ•´é«”ï¼ˆä¾è¦å‰‡æ‰£ä¼‘ï¼‰
    whole_first, whole_last, day_cnt = _span_metrics(times)
    if day_cnt > 0:
        whole_break, br_tag_whole = break_minutes_for_span(whole_first, whole_last)
        raw_whole_mins = (whole_last - whole_first).total_seconds() / 60.0
        whole_mins = max(int(round(raw_whole_mins - whole_break)), 0)
    else:
        whole_break, br_tag_whole, whole_mins = 0, "ç„¡æ™‚é–“è³‡æ–™", 0
    whole_eff = round((day_cnt / whole_mins * 60.0), 2) if whole_mins > 0 else 0.0

    return pd.Series({
        "ç¬¬ä¸€ç­†æ™‚é–“": whole_first, "æœ€å¾Œä¸€ç­†æ™‚é–“": whole_last, "ç•¶æ—¥ç­†æ•¸": int(day_cnt),
        "ä¼‘æ¯åˆ†é˜_æ•´é«”": int(whole_break), "å‘½ä¸­è¦å‰‡": br_tag_whole,
        "ç•¶æ—¥å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘": int(whole_mins), "æ•ˆç‡_ä»¶æ¯å°æ™‚": whole_eff,

        "ä¸Šåˆ_ç¬¬ä¸€ç­†": am_first, "ä¸Šåˆ_æœ€å¾Œä¸€ç­†": am_last, "ä¸Šåˆ_ç­†æ•¸": int(am_cnt),
        "ä¸Šåˆ_å·¥æ™‚_åˆ†é˜": int(am_mins), "ä¸Šåˆ_æ•ˆç‡_ä»¶æ¯å°æ™‚": am_eff,
        "ä¸Šåˆ_ç©ºçª—åˆ†é˜": int(am_idle_min), "ä¸Šåˆ_ç©ºçª—æ™‚æ®µ": am_idle_ranges,

        "ä¸‹åˆ_ç¬¬ä¸€ç­†": pm_first, "ä¸‹åˆ_æœ€å¾Œä¸€ç­†": pm_last, "ä¸‹åˆ_ç­†æ•¸": int(pm_cnt),
        "ä¸‹åˆ_ä¼‘æ¯åˆ†é˜": int(pm_break), "ä¸‹åˆ_å‘½ä¸­è¦å‰‡": pm_rule,
        "ä¸‹åˆ_å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘": int(pm_mins), "ä¸‹åˆ_æ•ˆç‡_ä»¶æ¯å°æ™‚": pm_eff,
        "ä¸‹åˆ_ç©ºçª—åˆ†é˜_æ‰£ä¼‘": int(pm_idle_min), "ä¸‹åˆ_ç©ºçª—æ™‚æ®µ": pm_idle_ranges,
    })

def _eff(n, m):
    return round((n / m * 60.0), 2) if m and m > 0 else 0.0


# =========================
# è®€æª”ï¼ˆStreamlit in-memoryï¼‰
# =========================
def read_excel_any_quiet_bytes(name: str, content: bytes) -> Dict[str, pd.DataFrame]:
    ext = (name.split(".")[-1] or "").lower()
    if ext in ("xlsx", "xlsm"):
        xl = pd.ExcelFile(io.BytesIO(content), engine="openpyxl")
        return {sn: pd.read_excel(xl, sheet_name=sn) for sn in xl.sheet_names}
    if ext == "csv":
        for enc in ("utf-8-sig", "cp950", "big5"):
            try:
                return {"CSV": pd.read_csv(io.BytesIO(content), encoding=enc)}
            except Exception:
                continue
        raise Exception("CSV è®€å–å¤±æ•—ï¼ˆè«‹ç¢ºèªç·¨ç¢¼ï¼‰ã€‚")
    raise Exception("ç›®å‰ Streamlit åƒ…æ”¯æ´ .xlsx/.xlsm/.csvï¼ˆè«‹å¦å­˜ç‚º xlsx å¾Œå†ä¸Šå‚³ï¼‰ã€‚")


# =========================
# åŒ¯å‡º Excelï¼ˆbytesï¼‰
# =========================
def shade_rows_by_efficiency(ws, header_name="æ•ˆç‡_ä»¶æ¯å°æ™‚", green="C6EFCE", red="FFC7CE"):
    from openpyxl.styles import PatternFill
    eff_col = None
    for c in range(1, ws.max_column + 1):
        if str(ws.cell(row=1, column=c).value).strip() == header_name:
            eff_col = c
            break
    if eff_col is None:
        return
    green_fill = PatternFill(start_color=green, end_color=green, fill_type="solid")
    red_fill = PatternFill(start_color=red, end_color=red, fill_type="solid")
    for r in range(2, ws.max_row + 1):
        v = ws.cell(row=r, column=eff_col).value
        try:
            val = float(v) if v is not None and str(v).strip() != "" else None
        except Exception:
            val = None
        if val is None:
            continue
        fill = green_fill if val >= TARGET_EFF else red_fill
        for c in range(1, ws.max_column + 1):
            ws.cell(row=r, column=c).fill = fill

def autosize_columns(ws, df: pd.DataFrame):
    from openpyxl.utils import get_column_letter
    cols = list(df.columns) if df is not None else []
    for i, col in enumerate(cols, start=1):
        if df is not None and not df.empty:
            sample = [len(str(x)) for x in df[col].head(1000).tolist()]
            max_len = max([len(str(col))] + sample)
        else:
            max_len = max(len(str(col)), 8)
        ws.column_dimensions[get_column_letter(i)].width = min(max_len + 2, 60)

def write_block_report(writer, detail_long: pd.DataFrame, user_col: str):
    import openpyxl
    from openpyxl.styles import Font, Alignment, PatternFill, Border, Side
    from openpyxl.utils import get_column_letter

    sheet_name = "å ±è¡¨_å€å¡Š"
    wb = writer.book
    if sheet_name in wb.sheetnames:
        del wb[sheet_name]
    ws = wb.create_sheet(sheet_name)

    header = ["ä»£ç¢¼", "å§“å", "ç­†æ•¸", "å·¥ä½œå€é–“", "ç¸½åˆ†é˜", "æ•ˆç‡(ä»¶/æ™‚)", "ä¼‘æ¯åˆ†é˜", "ç©ºçª—åˆ†é˜", "ç©ºçª—æ™‚æ®µ"]
    title_font = Font(bold=True, size=14)
    sec_font = Font(bold=True, size=12)
    header_fill = PatternFill(start_color="D9D9D9", end_color="D9D9D9", fill_type="solid")
    border = Border(left=Side(style="thin"), right=Side(style="thin"), top=Side(style="thin"), bottom=Side(style="thin"))
    center = Alignment(horizontal="center", vertical="center")
    left = Alignment(horizontal="left", vertical="center")

    df = detail_long.copy()
    df["å·¥ä½œå€é–“"] = df.apply(
        lambda r: (("" if pd.isna(r["ç¬¬ä¸€ç­†æ™‚é–“"]) else str(r["ç¬¬ä¸€ç­†æ™‚é–“"].time())) + " ~ " +
                   ("" if pd.isna(r["æœ€å¾Œä¸€ç­†æ™‚é–“"]) else str(r["æœ€å¾Œä¸€ç­†æ™‚é–“"].time()))),
        axis=1
    )
    df["ç¸½åˆ†é˜"] = df["å·¥æ™‚_åˆ†é˜"].astype(int)

    for dt_date, g in df.groupby("æ—¥æœŸ"):
        row = ws.max_row + 1
        ws.merge_cells(start_row=row, start_column=1, end_row=row, end_column=len(header))
        cell = ws.cell(row=row, column=1, value=f"{dt_date} ä¸Šæ¶ç¸¾æ•ˆ")
        cell.font = title_font
        cell.alignment = center

        for seg in ["ä¸Šåˆ", "ä¸‹åˆ"]:
            seg_df = g[g["æ™‚æ®µ"] == seg]
            if seg_df.empty:
                continue

            row = ws.max_row + 1
            ws.merge_cells(start_row=row, start_column=1, end_row=row, end_column=len(header))
            cell = ws.cell(row=row, column=1, value=seg)
            cell.font = sec_font
            cell.alignment = left

            row = ws.max_row + 1
            for c, h in enumerate(header, start=1):
                cell = ws.cell(row=row, column=c, value=h)
                cell.fill = header_fill
                cell.alignment = center
                cell.border = border
                cell.font = Font(bold=True)

            seg_df = seg_df.sort_values(["æ•ˆç‡_ä»¶æ¯å°æ™‚", "ç­†æ•¸"], ascending=[False, False])
            for _, r in seg_df.iterrows():
                row = ws.max_row + 1
                values = [
                    r[user_col],
                    r["å°æ‡‰å§“å"],
                    int(r["ç­†æ•¸"]),
                    r["å·¥ä½œå€é–“"],
                    int(r["ç¸½åˆ†é˜"]),
                    float(r["æ•ˆç‡_ä»¶æ¯å°æ™‚"]),
                    int(r["ä¼‘æ¯åˆ†é˜"]),
                    int(r["ç©ºçª—åˆ†é˜"]),
                    r["ç©ºçª—æ™‚æ®µ"],
                ]
                for c, v in enumerate(values, start=1):
                    cell = ws.cell(row=row, column=c, value=v)
                    cell.alignment = center if c not in (4, 9) else left
                    cell.border = border

                eff = float(r["æ•ˆç‡_ä»¶æ¯å°æ™‚"]) if pd.notna(r["æ•ˆç‡_ä»¶æ¯å°æ™‚"]) else 0.0
                color = "C6EFCE" if eff >= TARGET_EFF else "FFC7CE"
                fill = PatternFill(start_color=color, end_color=color, fill_type="solid")
                for c in range(1, len(header) + 1):
                    ws.cell(row=row, column=c).fill = fill

    for c in range(1, len(header) + 1):
        max_len = max(
            len(str(header[c - 1])),
            max((len(str(ws.cell(row=r, column=c).value)) for r in range(1, ws.max_row + 1)), default=0),
        )
        ws.column_dimensions[get_column_letter(c)].width = min(max_len + 2, 60)

def build_excel_bytes(user_col: str, summary_out: pd.DataFrame, daily: pd.DataFrame, detail_long: pd.DataFrame) -> bytes:
    out = io.BytesIO()
    with pd.ExcelWriter(out, engine="openpyxl", datetime_format="yyyy-mm-dd hh:mm:ss", date_format="yyyy-mm-dd") as writer:
        # å½™ç¸½
        sum_cols = [
            user_col, "å°æ‡‰å§“å", "ç·æ—¥æ•¸",
            "ç¸½ç­†æ•¸", "ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "æ•ˆç‡_ä»¶æ¯å°æ™‚",
            "ä¸Šåˆç­†æ•¸", "ä¸Šåˆå·¥æ™‚_åˆ†é˜", "ä¸Šåˆæ•ˆç‡_ä»¶æ¯å°æ™‚",
            "ä¸‹åˆç­†æ•¸", "ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "ä¸‹åˆæ•ˆç‡_ä»¶æ¯å°æ™‚",
        ]
        summary_out[sum_cols].to_excel(writer, index=False, sheet_name="å½™ç¸½")
        ws_sum = writer.sheets["å½™ç¸½"]
        autosize_columns(ws_sum, summary_out[sum_cols])
        shade_rows_by_efficiency(ws_sum, "æ•ˆç‡_ä»¶æ¯å°æ™‚")

        # æ˜ç´°ï¼ˆä¸è¼¸å‡ºå‘½ä¸­è¦å‰‡ï¼‰
        det_cols = [
            user_col, "å°æ‡‰å§“å", "æ—¥æœŸ",
            "ç¬¬ä¸€ç­†æ™‚é–“", "æœ€å¾Œä¸€ç­†æ™‚é–“", "ç•¶æ—¥ç­†æ•¸",
            "ä¼‘æ¯åˆ†é˜_æ•´é«”", "ç•¶æ—¥å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "æ•ˆç‡_ä»¶æ¯å°æ™‚",
            "ä¸Šåˆ_ç¬¬ä¸€ç­†", "ä¸Šåˆ_æœ€å¾Œä¸€ç­†", "ä¸Šåˆ_ç­†æ•¸", "ä¸Šåˆ_å·¥æ™‚_åˆ†é˜", "ä¸Šåˆ_æ•ˆç‡_ä»¶æ¯å°æ™‚",
            "ä¸Šåˆ_ç©ºçª—åˆ†é˜", "ä¸Šåˆ_ç©ºçª—æ™‚æ®µ",
            "ä¸‹åˆ_ç¬¬ä¸€ç­†", "ä¸‹åˆ_æœ€å¾Œä¸€ç­†", "ä¸‹åˆ_ç­†æ•¸", "ä¸‹åˆ_ä¼‘æ¯åˆ†é˜",
            "ä¸‹åˆ_å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "ä¸‹åˆ_æ•ˆç‡_ä»¶æ¯å°æ™‚",
            "ä¸‹åˆ_ç©ºçª—åˆ†é˜_æ‰£ä¼‘", "ä¸‹åˆ_ç©ºçª—æ™‚æ®µ",
        ]
        daily.sort_values([user_col, "æ—¥æœŸ", "ç¬¬ä¸€ç­†æ™‚é–“"])[det_cols].to_excel(writer, index=False, sheet_name="æ˜ç´°")
        ws_det = writer.sheets["æ˜ç´°"]
        autosize_columns(ws_det, daily[det_cols])
        shade_rows_by_efficiency(ws_det, "æ•ˆç‡_ä»¶æ¯å°æ™‚")

        # æ˜ç´°_æ™‚æ®µï¼ˆé•·è¡¨ï¼›ä¿ç•™å‘½ä¸­è¦å‰‡ï¼‰
        if detail_long is not None and not detail_long.empty:
            long_cols = [
                user_col, "å°æ‡‰å§“å", "æ—¥æœŸ", "æ™‚æ®µ",
                "ç¬¬ä¸€ç­†æ™‚é–“", "æœ€å¾Œä¸€ç­†æ™‚é–“",
                "ç­†æ•¸", "å·¥æ™‚_åˆ†é˜", "ä¼‘æ¯åˆ†é˜",
                "ç©ºçª—åˆ†é˜", "ç©ºçª—æ™‚æ®µ",
                "æ•ˆç‡_ä»¶æ¯å°æ™‚", "å‘½ä¸­è¦å‰‡",
            ]
            detail_long[long_cols].to_excel(writer, index=False, sheet_name="æ˜ç´°_æ™‚æ®µ")
            ws_long = writer.sheets["æ˜ç´°_æ™‚æ®µ"]
            autosize_columns(ws_long, detail_long[long_cols])
            shade_rows_by_efficiency(ws_long, "æ•ˆç‡_ä»¶æ¯å°æ™‚")

            # å ±è¡¨_å€å¡Šï¼ˆç„¡å‘½ä¸­è¦å‰‡æ¬„ï¼‰
            write_block_report(writer, detail_long, user_col)

        # ä¼‘æ¯è¦å‰‡
        rules_rows = []
        for i, (st_ge, ed_le, mins, tag) in enumerate(BREAK_RULES, start=1):
            rules_rows.append({
                "å„ªå…ˆåº": i,
                "é¦–æ™‚é–“æ¢ä»¶(>=)": st_ge.strftime("%H:%M:%S"),
                "æœ«æ™‚é–“æ¢ä»¶(<=)": ed_le.strftime("%H:%M:%S"),
                "ä¼‘æ¯åˆ†é˜": int(mins),
                "è¦å‰‡èªªæ˜": str(tag),
            })
        rules_df = pd.DataFrame(rules_rows, columns=["å„ªå…ˆåº", "é¦–æ™‚é–“æ¢ä»¶(>=)", "æœ«æ™‚é–“æ¢ä»¶(<=)", "ä¼‘æ¯åˆ†é˜", "è¦å‰‡èªªæ˜"])
        rules_df.to_excel(writer, index=False, sheet_name="ä¼‘æ¯è¦å‰‡")
        autosize_columns(writer.sheets["ä¼‘æ¯è¦å‰‡"], rules_df)

    return out.getvalue()


# =========================
# Streamlit Page
# =========================
def main():
    inject_logistics_theme()
    set_page("ä¸Šæ¶ç”¢èƒ½åˆ†æï¼ˆPutaway KPIï¼‰", icon="ğŸ“¦")
    st.caption("ç¸½ä¸Šçµ„ï¼ˆä¸Šæ¶ï¼‰ï½œä¾åŸå§‹ QC ç´€éŒ„è¨ˆç®—ï½œAM/PM ç­åˆ¥ï¼ˆä¸Šåˆ/ä¸‹åˆï¼‰ï½œç©ºçª—æ’é™¤å›ºå®šå¸¶ï½œä¸‹åˆæ‰£ä¼‘")

    with st.sidebar:
        st.header("âš™ï¸ è¨ˆç®—æ¢ä»¶è¨­å®š")
        operator = st.text_input("åˆ†æåŸ·è¡Œäººï¼ˆOperatorï¼‰")
        top_n = st.number_input("æ•ˆç‡æ’è¡Œé¡¯ç¤ºäººæ•¸ï¼ˆTop Nï¼‰", 10, 100, 30, step=5)
        st.info("è¦å‰‡ä¾†æºï¼šv8.9 ä¸Šæ¶é”æ¨™åˆ†ä¸Šä¸‹åˆå€æ®µï¼ˆå·²ç§»æ¤ï¼‰ã€‚")

    card_open("ğŸ“¤ ä¸Šå‚³ä½œæ¥­åŸå§‹è³‡æ–™ï¼ˆä¸Šæ¶ï¼‰")
    uploaded = st.file_uploader(
        "ä¸Šå‚³ Excel / CSVï¼ˆåŒ…å«ã€ç”±/åˆ°ã€ã€ä¿®è¨‚æ—¥æœŸ/æ™‚é–“ã€ã€è¨˜éŒ„è¼¸å…¥äººã€ï¼‰",
        type=["xlsx", "xlsm", "csv"],
        label_visibility="collapsed",
    )
    run = st.button("ğŸš€ ç”¢å‡º KPI", type="primary", disabled=uploaded is None)
    card_close()

    if not run:
        st.info("è«‹å…ˆä¸Šå‚³ä¸Šæ¶ä½œæ¥­åŸå§‹è³‡æ–™")
        return

    # è®€æª” + éæ¿¾ + è¨ˆç®—
    with st.spinner("è¨ˆç®—ä¸­ï¼Œè«‹ç¨å€™..."):
        content = uploaded.getvalue()
        sheets = read_excel_any_quiet_bytes(uploaded.name, content)

        kept_all = []
        for sn, df in sheets.items():
            k = prepare_filtered_df(df)
            if not k.empty:
                k["__sheet__"] = sn
                kept_all.append(k)
        if not kept_all:
            st.error("ç„¡ç¬¦åˆè³‡æ–™ï¼ˆå¯èƒ½ç¼ºã€ç”±/åˆ°ã€æ¬„æˆ–éæ¿¾å¾Œç‚ºç©ºï¼‰ã€‚")
            return
        data = pd.concat(kept_all, ignore_index=True)

        user_col = find_first_column(data, INPUT_USER_CANDIDATES)
        revdt_col = find_first_column(data, REV_DT_CANDIDATES)
        if user_col is None:
            st.error("æ‰¾ä¸åˆ°ã€è¨˜éŒ„è¼¸å…¥äººã€æ¬„ä½ï¼ˆå€™é¸ï¼šè¨˜éŒ„è¼¸å…¥äºº/è¨˜éŒ„è¼¸å…¥è€…/å»ºç«‹äºº/è¼¸å…¥äººï¼‰ã€‚")
            return
        if revdt_col is None:
            st.error("æ‰¾ä¸åˆ°ã€ä¿®è¨‚æ—¥æœŸ/æ™‚é–“ã€æ¬„ä½ï¼ˆå€™é¸ï¼šä¿®è¨‚æ—¥æœŸ/ä¿®è¨‚æ™‚é–“/ä¿®è¨‚æ—¥/ç•°å‹•æ™‚é–“/ä¿®æ”¹æ™‚é–“ï¼‰ã€‚")
            return

        data["__dt__"] = pd.to_datetime(data[revdt_col], errors="coerce")
        data["__code__"] = data[user_col].astype(str).str.strip()
        data["å°æ‡‰å§“å"] = data["__code__"].map(NAME_MAP).fillna("")

        dt_data = data.dropna(subset=["__dt__"]).copy()
        if dt_data.empty:
            st.error("è³‡æ–™æ²’æœ‰å¯ç”¨çš„ä¿®è¨‚æ—¥æœŸæ™‚é–“ï¼Œç„¡æ³•è¨ˆç®—ã€‚")
            return

        dt_data["æ—¥æœŸ"] = dt_data["__dt__"].dt.date

        daily = (
            dt_data.groupby([user_col, "å°æ‡‰å§“å", "æ—¥æœŸ"], dropna=False)
                   .apply(compute_am_pm_for_group)
                   .reset_index()
        )

        summary = (
            daily.groupby([user_col, "å°æ‡‰å§“å"], dropna=False, as_index=False)
                 .agg(
                     ç·æ—¥æ•¸=("æ—¥æœŸ", "nunique"),
                     ç¸½ç­†æ•¸=("ç•¶æ—¥ç­†æ•¸", "sum"),
                     ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘=("ç•¶æ—¥å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "sum"),
                     ä¸Šåˆç­†æ•¸=("ä¸Šåˆ_ç­†æ•¸", "sum"),
                     ä¸Šåˆå·¥æ™‚_åˆ†é˜=("ä¸Šåˆ_å·¥æ™‚_åˆ†é˜", "sum"),
                     ä¸‹åˆç­†æ•¸=("ä¸‹åˆ_ç­†æ•¸", "sum"),
                     ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘=("ä¸‹åˆ_å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "sum"),
                 )
        )
        summary["ä¸Šåˆæ•ˆç‡_ä»¶æ¯å°æ™‚"] = summary.apply(lambda r: _eff(r["ä¸Šåˆç­†æ•¸"], r["ä¸Šåˆå·¥æ™‚_åˆ†é˜"]), axis=1)
        summary["ä¸‹åˆæ•ˆç‡_ä»¶æ¯å°æ™‚"] = summary.apply(lambda r: _eff(r["ä¸‹åˆç­†æ•¸"], r["ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"]), axis=1)
        summary["ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"] = summary["ä¸Šåˆå·¥æ™‚_åˆ†é˜"].fillna(0).astype(int) + summary["ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"].fillna(0).astype(int)
        summary["æ•ˆç‡_ä»¶æ¯å°æ™‚"] = summary.apply(lambda r: _eff(r["ç¸½ç­†æ•¸"], r["ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"]), axis=1)

        for c in ["ç¸½ç­†æ•¸", "ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "ä¸Šåˆç­†æ•¸", "ä¸Šåˆå·¥æ™‚_åˆ†é˜", "ä¸‹åˆç­†æ•¸", "ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"]:
            summary[c] = summary[c].fillna(0).astype(int)

        summary = summary.sort_values(["ç¸½ç­†æ•¸", "ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"], ascending=[False, False])

        # æ•´é«”åˆè¨ˆåˆ—
        total_people = int(summary[user_col].nunique())
        met_people = int((summary["æ•ˆç‡_ä»¶æ¯å°æ™‚"] >= TARGET_EFF).sum())
        rate = (met_people / total_people) if total_people > 0 else 0.0

        total_row = {
            user_col: "æ•´é«”åˆè¨ˆ", "å°æ‡‰å§“å": "",
            "ç·æ—¥æ•¸": int(summary["ç·æ—¥æ•¸"].sum()),
            "ç¸½ç­†æ•¸": int(summary["ç¸½ç­†æ•¸"].sum()),
            "ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘": int(summary["ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"].sum()),
            "ä¸Šåˆç­†æ•¸": int(summary["ä¸Šåˆç­†æ•¸"].sum()),
            "ä¸Šåˆå·¥æ™‚_åˆ†é˜": int(summary["ä¸Šåˆå·¥æ™‚_åˆ†é˜"].sum()),
            "ä¸‹åˆç­†æ•¸": int(summary["ä¸‹åˆç­†æ•¸"].sum()),
            "ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘": int(summary["ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"].sum()),
            "æ•ˆç‡_ä»¶æ¯å°æ™‚": _eff(int(summary["ç¸½ç­†æ•¸"].sum()), int(summary["ç¸½å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"].sum())),
            "ä¸Šåˆæ•ˆç‡_ä»¶æ¯å°æ™‚": _eff(int(summary["ä¸Šåˆç­†æ•¸"].sum()), int(summary["ä¸Šåˆå·¥æ™‚_åˆ†é˜"].sum())),
            "ä¸‹åˆæ•ˆç‡_ä»¶æ¯å°æ™‚": _eff(int(summary["ä¸‹åˆç­†æ•¸"].sum()), int(summary["ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"].sum())),
        }
        summary_out = pd.concat([summary, pd.DataFrame([total_row])], ignore_index=True)

        # æ˜ç´°_æ™‚æ®µï¼ˆé•·è¡¨ï¼‰
        long_rows = []
        for _, r in daily.iterrows():
            if r["ä¸Šåˆ_ç­†æ•¸"] > 0:
                long_rows.append({
                    user_col: r[user_col], "å°æ‡‰å§“å": r["å°æ‡‰å§“å"], "æ—¥æœŸ": r["æ—¥æœŸ"], "æ™‚æ®µ": "ä¸Šåˆ",
                    "ç¬¬ä¸€ç­†æ™‚é–“": r["ä¸Šåˆ_ç¬¬ä¸€ç­†"], "æœ€å¾Œä¸€ç­†æ™‚é–“": r["ä¸Šåˆ_æœ€å¾Œä¸€ç­†"],
                    "ç­†æ•¸": int(r["ä¸Šåˆ_ç­†æ•¸"]),
                    "å·¥æ™‚_åˆ†é˜": int(r["ä¸Šåˆ_å·¥æ™‚_åˆ†é˜"]),
                    "ä¼‘æ¯åˆ†é˜": 0,
                    "ç©ºçª—åˆ†é˜": int(r["ä¸Šåˆ_ç©ºçª—åˆ†é˜"]),
                    "ç©ºçª—æ™‚æ®µ": r["ä¸Šåˆ_ç©ºçª—æ™‚æ®µ"],
                    "æ•ˆç‡_ä»¶æ¯å°æ™‚": float(r["ä¸Šåˆ_æ•ˆç‡_ä»¶æ¯å°æ™‚"]),
                    "å‘½ä¸­è¦å‰‡": "ä¸Šåˆä¸æ‰£ä¼‘",
                })
            if r["ä¸‹åˆ_ç­†æ•¸"] > 0:
                long_rows.append({
                    user_col: r[user_col], "å°æ‡‰å§“å": r["å°æ‡‰å§“å"], "æ—¥æœŸ": r["æ—¥æœŸ"], "æ™‚æ®µ": "ä¸‹åˆ",
                    "ç¬¬ä¸€ç­†æ™‚é–“": r["ä¸‹åˆ_ç¬¬ä¸€ç­†"], "æœ€å¾Œä¸€ç­†æ™‚é–“": r["ä¸‹åˆ_æœ€å¾Œä¸€ç­†"],
                    "ç­†æ•¸": int(r["ä¸‹åˆ_ç­†æ•¸"]),
                    "å·¥æ™‚_åˆ†é˜": int(r["ä¸‹åˆ_å·¥æ™‚_åˆ†é˜_æ‰£ä¼‘"]),
                    "ä¼‘æ¯åˆ†é˜": int(r["ä¸‹åˆ_ä¼‘æ¯åˆ†é˜"]),
                    "ç©ºçª—åˆ†é˜": int(r["ä¸‹åˆ_ç©ºçª—åˆ†é˜_æ‰£ä¼‘"]),
                    "ç©ºçª—æ™‚æ®µ": r["ä¸‹åˆ_ç©ºçª—æ™‚æ®µ"],
                    "æ•ˆç‡_ä»¶æ¯å°æ™‚": float(r["ä¸‹åˆ_æ•ˆç‡_ä»¶æ¯å°æ™‚"]),
                    "å‘½ä¸­è¦å‰‡": str(r["ä¸‹åˆ_å‘½ä¸­è¦å‰‡"]),
                })
        detail_long = pd.DataFrame(long_rows)
        if not detail_long.empty:
            detail_long = detail_long.sort_values([user_col, "æ—¥æœŸ", "æ™‚æ®µ", "ç¬¬ä¸€ç­†æ™‚é–“"])

        # ç”¢å‡º Excel bytes
        xlsx_bytes = build_excel_bytes(user_col, summary_out, daily, detail_long)

    # ======================
    # UIï¼šAM/PM å·¦å³åˆ†å€ï¼ˆç”¨ä½ è¦çš„å°ˆæ¥­è¡“èªï¼‰
    # ======================
    # ä»¥ã€Œå½™ç¸½ã€çš„ä¸Šåˆ/ä¸‹åˆæ•ˆç‡ä½œåœ–
    plot_df = summary.copy()
    plot_df = plot_df[plot_df[user_col] != "æ•´é«”åˆè¨ˆ"] if (plot_df[user_col] == "æ•´é«”åˆè¨ˆ").any() else plot_df

    col_l, col_r = st.columns(2)

    with col_l:
        card_open("ğŸŒ“ AM ç­ï¼ˆä¸Šåˆï¼‰KPI")
        render_kpis([
            KPI("ç¸½äººæ•¸", f"{total_people:,}"),
            KPI("é”æ¨™äººæ•¸", f"{met_people:,}"),
            KPI("é”æ¨™ç‡", f"{rate:.1%}"),
            KPI("é”æ¨™é–€æª»", f"æ•ˆç‡ â‰¥ {TARGET_EFF}"),
        ])
        card_close()

        card_open(f"AM ç­ï¼ˆä¸Šåˆï¼‰æ•ˆç‡æ’è¡Œï¼ˆTop {int(top_n)}ï¼‰")
        am_rank = plot_df[[user_col, "å°æ‡‰å§“å", "ä¸Šåˆç­†æ•¸", "ä¸Šåˆå·¥æ™‚_åˆ†é˜", "ä¸Šåˆæ•ˆç‡_ä»¶æ¯å°æ™‚"]].copy()
        am_rank = am_rank.rename(columns={"ä¸Šåˆæ•ˆç‡_ä»¶æ¯å°æ™‚": "æ•ˆç‡", "ä¸Šåˆç­†æ•¸": "ç­†æ•¸", "ä¸Šåˆå·¥æ™‚_åˆ†é˜": "å·¥æ™‚"})
        am_rank["å§“å"] = am_rank["å°æ‡‰å§“å"].where(am_rank["å°æ‡‰å§“å"].astype(str).str.len() > 0, am_rank[user_col].astype(str))
        bar_topN(
            am_rank[["å§“å", "æ•ˆç‡", "ç­†æ•¸", "å·¥æ™‚"]],
            x_col="å§“å", y_col="æ•ˆç‡",
            hover_cols=["ç­†æ•¸", "å·¥æ™‚"],
            top_n=int(top_n),
            target=float(TARGET_EFF),
        )
        card_close()

    with col_r:
        card_open("ğŸŒ™ PM ç­ï¼ˆä¸‹åˆï¼‰KPI")
        # ä¸‹åˆé”æ¨™ç‡ç”¨ã€Œä¸‹åˆæ•ˆç‡ã€
        pm_met = int((plot_df["ä¸‹åˆæ•ˆç‡_ä»¶æ¯å°æ™‚"] >= TARGET_EFF).sum())
        pm_total = int(plot_df[user_col].nunique())
        pm_rate = (pm_met / pm_total) if pm_total > 0 else 0.0
        render_kpis([
            KPI("ç¸½äººæ•¸", f"{pm_total:,}"),
            KPI("é”æ¨™äººæ•¸", f"{pm_met:,}"),
            KPI("é”æ¨™ç‡", f"{pm_rate:.1%}"),
            KPI("é”æ¨™é–€æª»", f"æ•ˆç‡ â‰¥ {TARGET_EFF}"),
        ])
        card_close()

        card_open(f"PM ç­ï¼ˆä¸‹åˆï¼‰æ•ˆç‡æ’è¡Œï¼ˆTop {int(top_n)}ï¼‰")
        pm_rank = plot_df[[user_col, "å°æ‡‰å§“å", "ä¸‹åˆç­†æ•¸", "ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘", "ä¸‹åˆæ•ˆç‡_ä»¶æ¯å°æ™‚"]].copy()
        pm_rank = pm_rank.rename(columns={"ä¸‹åˆæ•ˆç‡_ä»¶æ¯å°æ™‚": "æ•ˆç‡", "ä¸‹åˆç­†æ•¸": "ç­†æ•¸", "ä¸‹åˆå·¥æ™‚_åˆ†é˜_æ‰£ä¼‘": "å·¥æ™‚"})
        pm_rank["å§“å"] = pm_rank["å°æ‡‰å§“å"].where(pm_rank["å°æ‡‰å§“å"].astype(str).str.len() > 0, pm_rank[user_col].astype(str))
        bar_topN(
            pm_rank[["å§“å", "æ•ˆç‡", "ç­†æ•¸", "å·¥æ™‚"]],
            x_col="å§“å", y_col="æ•ˆç‡",
            hover_cols=["ç­†æ•¸", "å·¥æ™‚"],
            top_n=int(top_n),
            target=float(TARGET_EFF),
        )
        card_close()

    # åŒ¯å‡º
    card_open("â¬‡ï¸ åŒ¯å‡º KPI å ±è¡¨ï¼ˆExcelï¼‰")
    default_name = f"{uploaded.name.rsplit('.',1)[0]}ä¸Šæ¶ç¸¾æ•ˆ.xlsx"
    download_excel(xlsx_bytes, default_name)
    card_close()

    # ç¨½æ ¸ç•™å­˜ï¼ˆDB + Storageï¼‰
    st.divider()
    st.subheader("ğŸ§¾ ç¨½æ ¸ç•™å­˜ç‹€æ…‹")
    try:
        export_path = upload_export_bytes(
            content=xlsx_bytes,
            object_path=f"putaway_runs/{dt.datetime.now():%Y%m%d}/{uuid.uuid4().hex}_putaway.xlsx",
        )
        payload = {
            "app_name": "ä¸Šæ¶ç”¢èƒ½åˆ†æï¼ˆPutaway KPIï¼‰",
            "operator": operator or None,
            "source_filename": uploaded.name,
            "source_sha256": sha256_bytes(content),
            "params": {
                "top_n": int(top_n),
                "target_eff": TARGET_EFF,
                "filter": "ç”±=QC ä¸” åˆ°ä¸å«é—œéµå­—",
                "am_range": "07:00-12:30",
                "pm_range": "13:30-23:59:59",
            },
            "kpi_am": {"people": total_people, "pass_rate": rate},
            "kpi_pm": {"people": pm_total, "pass_rate": pm_rate},
            "export_object_path": export_path,
        }
        row = insert_audit_run(payload)
        st.success(f"âœ… å·²æˆåŠŸç•™å­˜æœ¬æ¬¡åˆ†æï¼ˆIDï¼š{row.get('id','')}ï¼‰")
    except Exception as e:
        st.error("âŒ ç¨½æ ¸ç•™å­˜ç™¼ç”ŸéŒ¯èª¤")
        st.code(repr(e))


if __name__ == "__main__":
    main()
